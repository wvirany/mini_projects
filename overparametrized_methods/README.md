The purpose of this notebook was to investigate the idea that interpolating classifiers perform well when averaged together. Evidently, this was not the case. However, the literature indicates that certain overparametrized methods, e.g., AdaBoost and Random Forests (see Wyner et al. 2017), which interpolate the data are robust to noise. What is the difference?
